# Note-Book

## Google Dorking

What are Crawlers?
```
Q.1: Name the key term of what a "Crawler" is used to do
A: index
Q.2: What is the name of the technique that "Search Engines" use to retrieve this information about websites? 
A: crawling
Q.2: What is an example of the type of contents that could be gathered from a website?
A: keywords
```
Search Engine Optimisation (SEO)
```
https://web.dev/measure/
https://pagespeed.web.dev/
https://seositecheckup.com/
https://neilpatel.com/seo-analyzer/
```
Robots.txt
```
Q.1: Where would "robots.txt" be located on the domain "ablog.com"
A: ablog.com/robots.txt
Q.2: If a website was to have a sitemap, where would that be located?
A: /sitemap.xml
Q.3: How would we only allow "Bingbot" to index the website?
A: User-agent:Bingbot
Q.4: How would we prevent a "Crawler" from indexing the directory "/dont-index-me/"?
A: Disallow:/dont-index-me/
Q.5: What is the extension of a Unix/Linux system configuration file that we might want to hide from "Crawlers"?
A: .conf
```
Sitemaps
```
Q.1: What is the typical file structure of a "Sitemap"?
A: xml
Q.2: What real life example can "Sitemaps" be compared to?
A: map
Q.3: Name the keyword for the path taken for content on a website
A: route
```
Using Google for Advanced Searching
```
Q.1 : What would be the format used to query the site bbc.co.uk about flood defences
A: site:bbc.co.uk flood defences
Q.2 : What term would you use to search by file type?
A: filetype:
Q.3 : What term can we use to look for login pages?
A: intitle:login
```

